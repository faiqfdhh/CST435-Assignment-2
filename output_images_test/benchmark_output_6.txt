============================================================
IMAGE PROCESSING PARALLELISM BENCHMARK
============================================================
Images: 1 | CPU Cores: 12
Input: ./input_images_test | Output: ./output_images_test

Pre-loading images...

------------------------------------------------------------
Step 1: Sequential Processing
------------------------------------------------------------
Total time: 7.7207s | Avg per image: 7.7207s

------------------------------------------------------------
Step 2: Parallel Benchmarks
------------------------------------------------------------

========================================
Testing with 1 worker(s)
========================================

[Pixel-level]
Multiprocessing...
  Time: 7.4279s | Speedup: 1.04x | Efficiency: 103.94%
Futures...
  Time: 6.9590s | Speedup: 1.11x | Efficiency: 110.95%

[Image-level]
Multiprocessing...
  Time: 6.7771s | Speedup: 1.14x | Efficiency: 113.92%
Futures...
  Time: 7.0936s | Speedup: 1.09x | Efficiency: 108.84%

[Task-level]
Multiprocessing...
  Time: 7.0056s | Speedup: 1.10x | Efficiency: 110.21%
Futures...
  Time: 7.4574s | Speedup: 1.04x | Efficiency: 103.53%

========================================
Testing with 2 worker(s)
========================================

[Pixel-level]
Multiprocessing...
  Time: 4.2113s | Speedup: 1.83x | Efficiency: 91.67%
Futures...
  Time: 4.5251s | Speedup: 1.71x | Efficiency: 85.31%

[Image-level]
Multiprocessing...
  Time: 7.3238s | Speedup: 1.05x | Efficiency: 52.71%
Futures...
  Time: 6.8519s | Speedup: 1.13x | Efficiency: 56.34%

[Task-level]
Multiprocessing...
  Time: 7.0997s | Speedup: 1.09x | Efficiency: 54.37%
Futures...
  Time: 7.1117s | Speedup: 1.09x | Efficiency: 54.28%

========================================
Testing with 4 worker(s)
========================================

[Pixel-level]
Multiprocessing...
  Time: 2.6731s | Speedup: 2.89x | Efficiency: 72.21%
Futures...
  Time: 2.8600s | Speedup: 2.70x | Efficiency: 67.49%

[Image-level]
Multiprocessing...
  Time: 6.8447s | Speedup: 1.13x | Efficiency: 28.20%
Futures...
  Time: 6.9106s | Speedup: 1.12x | Efficiency: 27.93%

[Task-level]
Multiprocessing...
  Time: 7.3711s | Speedup: 1.05x | Efficiency: 26.19%
Futures...
  Time: 7.0205s | Speedup: 1.10x | Efficiency: 27.49%

========================================
Testing with 8 worker(s)
========================================

[Pixel-level]
Multiprocessing...
  Time: 2.3502s | Speedup: 3.29x | Efficiency: 41.06%
Futures...
  Time: 2.2343s | Speedup: 3.46x | Efficiency: 43.20%

[Image-level]
Multiprocessing...
  Time: 7.1896s | Speedup: 1.07x | Efficiency: 13.42%
Futures...
  Time: 7.1475s | Speedup: 1.08x | Efficiency: 13.50%

[Task-level]
Multiprocessing...
  Time: 7.3694s | Speedup: 1.05x | Efficiency: 13.10%
Futures...
  Time: 6.9854s | Speedup: 1.11x | Efficiency: 13.82%

================================================================================
PHASE 3: AMDAHL'S LAW & SCALABILITY ANALYSIS
================================================================================

================================================================================
PIXEL-LEVEL STRATEGY
================================================================================

Multiprocessing:
------------------------------------------------------------

Observed at 8 workers:
  Speedup: 3.29x
  Efficiency: 41.06%

Amdahl's Law Decomposition:
  Parallelizable portion (p): 79.50%
  Serial portion (1-p):       20.50%

Theoretical Limits (Amdahl's Ceiling):
  Maximum possible speedup (‚àû processors): 4.88x
  Current achievement: 67.4% of theoretical max

Projected Speedups (based on p=0.795):
Processors      Speedup         Efficiency     
16              3.93           x 24.54%         
32              4.35           x 13.59%         
64              4.60           x 7.19%          
128             4.73           x 3.70%          

Bottleneck Analysis:
  ‚ö†Ô∏è  MODERATE: 21% serial portion limits scaling
      Practical limit: ~4.9x speedup

Scaling Efficiency Trend:
  üìâ Degrading (60.5% loss from 1‚Üí8 workers)
  Efficiency curve: 104% ‚Üí 92% ‚Üí 72% ‚Üí 41%

Strong vs Weak Scaling:
  Strong Scaling: 3.16x (fixed workload)
    Ideal: 8x | Achieved: 39.5%
  Weak Scaling Estimate: 3.29x (if workload scaled with workers)
    ‚Üí Strong scaling constraints dominate (Amdahl's Law)

IPC & Process Overhead:
  Theoretical ideal: 0.9285s | Actual: 2.3502s
  Overhead: 1.4217s (60.5%)
  ‚ö†Ô∏è  HIGH: Serialization/pickling dominates computation

Concurrent.Futures:
------------------------------------------------------------

Observed at 8 workers:
  Speedup: 3.46x
  Efficiency: 43.20%

Amdahl's Law Decomposition:
  Parallelizable portion (p): 81.21%
  Serial portion (1-p):       18.79%

Theoretical Limits (Amdahl's Ceiling):
  Maximum possible speedup (‚àû processors): 5.32x
  Current achievement: 64.9% of theoretical max

Projected Speedups (based on p=0.812):
Processors      Speedup         Efficiency     
16              4.19           x 26.19%         
32              4.69           x 14.65%         
64              4.99           x 7.79%          
128             5.15           x 4.02%          

Bottleneck Analysis:
  ‚ö†Ô∏è  MODERATE: 19% serial portion limits scaling
      Practical limit: ~5.3x speedup

Scaling Efficiency Trend:
  üìâ Degrading (61.1% loss from 1‚Üí8 workers)
  Efficiency curve: 111% ‚Üí 85% ‚Üí 67% ‚Üí 43%

Strong vs Weak Scaling:
  Strong Scaling: 3.11x (fixed workload)
    Ideal: 8x | Achieved: 38.9%
  Weak Scaling Estimate: 3.46x (if workload scaled with workers)
    ‚Üí Strong scaling constraints dominate (Amdahl's Law)

IPC & Process Overhead:
  Theoretical ideal: 0.8699s | Actual: 2.2343s
  Overhead: 1.3644s (61.1%)
  ‚ö†Ô∏è  HIGH: Serialization/pickling dominates computation

================================================================================
IMAGE-LEVEL STRATEGY
================================================================================

Multiprocessing:
------------------------------------------------------------

Observed at 8 workers:
  Speedup: 1.07x
  Efficiency: 13.42%

Amdahl's Law Decomposition:
  Parallelizable portion (p): 7.86%
  Serial portion (1-p):       92.14%

Theoretical Limits (Amdahl's Ceiling):
  Maximum possible speedup (‚àû processors): 1.09x
  Current achievement: 98.9% of theoretical max

Projected Speedups (based on p=0.079):
Processors      Speedup         Efficiency     
16              1.08           x 6.75%          
32              1.08           x 3.38%          
64              1.08           x 1.69%          
128             1.08           x 0.85%          

Bottleneck Analysis:
  ‚ö†Ô∏è  CRITICAL: 92% of work is inherently serial
      Amdahl's ceiling is low (~1.1x max speedup)
      ‚Üí Load imbalance or process spawning overhead

Scaling Efficiency Trend:
  üìâ Degrading (88.2% loss from 1‚Üí8 workers)
  Efficiency curve: 114% ‚Üí 53% ‚Üí 28% ‚Üí 13%

Strong vs Weak Scaling:
  Strong Scaling: 0.94x (fixed workload)
    Ideal: 8x | Achieved: 11.8%
  Weak Scaling Estimate: 1.07x (if workload scaled with workers)
    ‚Üí Strong scaling constraints dominate (Amdahl's Law)

IPC & Process Overhead:
  Theoretical ideal: 0.8471s | Actual: 7.1896s
  Overhead: 6.3424s (88.2%)
  ‚ö†Ô∏è  HIGH: Serialization/pickling dominates computation

Concurrent.Futures:
------------------------------------------------------------

Observed at 8 workers:
  Speedup: 1.08x
  Efficiency: 13.50%

Amdahl's Law Decomposition:
  Parallelizable portion (p): 8.49%
  Serial portion (1-p):       91.51%

Theoretical Limits (Amdahl's Ceiling):
  Maximum possible speedup (‚àû processors): 1.09x
  Current achievement: 98.9% of theoretical max

Projected Speedups (based on p=0.085):
Processors      Speedup         Efficiency     
16              1.09           x 6.79%          
32              1.09           x 3.40%          
64              1.09           x 1.70%          
128             1.09           x 0.85%          

Bottleneck Analysis:
  ‚ö†Ô∏è  CRITICAL: 92% of work is inherently serial
      Amdahl's ceiling is low (~1.1x max speedup)
      ‚Üí Load imbalance or process spawning overhead

Scaling Efficiency Trend:
  üìâ Degrading (87.6% loss from 1‚Üí8 workers)
  Efficiency curve: 109% ‚Üí 56% ‚Üí 28% ‚Üí 14%

Strong vs Weak Scaling:
  Strong Scaling: 0.99x (fixed workload)
    Ideal: 8x | Achieved: 12.4%
  Weak Scaling Estimate: 1.08x (if workload scaled with workers)
    ‚Üí Strong scaling constraints dominate (Amdahl's Law)

IPC & Process Overhead:
  Theoretical ideal: 0.8867s | Actual: 7.1475s
  Overhead: 6.2608s (87.6%)
  ‚ö†Ô∏è  HIGH: Serialization/pickling dominates computation

================================================================================
TASK-LEVEL STRATEGY
================================================================================

Multiprocessing:
------------------------------------------------------------

Observed at 8 workers:
  Speedup: 1.05x
  Efficiency: 13.10%

Amdahl's Law Decomposition:
  Parallelizable portion (p): 5.20%
  Serial portion (1-p):       94.80%

Theoretical Limits (Amdahl's Ceiling):
  Maximum possible speedup (‚àû processors): 1.05x
  Current achievement: 99.3% of theoretical max

Projected Speedups (based on p=0.052):
Processors      Speedup         Efficiency     
16              1.05           x 6.57%          
32              1.05           x 3.29%          
64              1.05           x 1.65%          
128             1.05           x 0.82%          

Bottleneck Analysis:
  ‚ö†Ô∏è  CRITICAL: 95% of work is inherently serial
      Amdahl's ceiling is low (~1.1x max speedup)
      ‚Üí Pipeline stage imbalance or callback overhead

Scaling Efficiency Trend:
  üìâ Degrading (88.1% loss from 1‚Üí8 workers)
  Efficiency curve: 110% ‚Üí 54% ‚Üí 26% ‚Üí 13%

Strong vs Weak Scaling:
  Strong Scaling: 0.95x (fixed workload)
    Ideal: 8x | Achieved: 11.9%
  Weak Scaling Estimate: 1.05x (if workload scaled with workers)
    ‚Üí Strong scaling constraints dominate (Amdahl's Law)

IPC & Process Overhead:
  Theoretical ideal: 0.8757s | Actual: 7.3694s
  Overhead: 6.4937s (88.1%)
  ‚ö†Ô∏è  HIGH: Serialization/pickling dominates computation

Concurrent.Futures:
------------------------------------------------------------

Observed at 8 workers:
  Speedup: 1.11x
  Efficiency: 13.82%

Amdahl's Law Decomposition:
  Parallelizable portion (p): 10.88%
  Serial portion (1-p):       89.12%

Theoretical Limits (Amdahl's Ceiling):
  Maximum possible speedup (‚àû processors): 1.12x
  Current achievement: 98.5% of theoretical max

Projected Speedups (based on p=0.109):
Processors      Speedup         Efficiency     
16              1.11           x 6.96%          
32              1.12           x 3.49%          
64              1.12           x 1.75%          
128             1.12           x 0.88%          

Bottleneck Analysis:
  ‚ö†Ô∏è  CRITICAL: 89% of work is inherently serial
      Amdahl's ceiling is low (~1.1x max speedup)
      ‚Üí Pipeline stage imbalance or callback overhead

Scaling Efficiency Trend:
  üìâ Degrading (86.7% loss from 1‚Üí8 workers)
  Efficiency curve: 104% ‚Üí 54% ‚Üí 27% ‚Üí 14%

Strong vs Weak Scaling:
  Strong Scaling: 1.07x (fixed workload)
    Ideal: 8x | Achieved: 13.3%
  Weak Scaling Estimate: 1.11x (if workload scaled with workers)
    ‚Üí Strong scaling constraints dominate (Amdahl's Law)

IPC & Process Overhead:
  Theoretical ideal: 0.9322s | Actual: 6.9854s
  Overhead: 6.0532s (86.7%)
  ‚ö†Ô∏è  HIGH: Serialization/pickling dominates computation

================================================================================
PHASE 3B: STRATEGY COMPARISON & PRACTICAL IMPLICATIONS
================================================================================

Comparison at 8 workers:
Strategy             Speedup         Serial %        Amdahl Limit   
Pixel-Level          3.29           x 20.5           % 4.9x           
Image-Level          1.07           x 92.1           % 1.1x           
Task-Level           1.05           x 94.8           % 1.1x           

Recommendations:
  ‚Ä¢ Best theoretical ceiling: Pixel-Level (~4.9x max speedup)
  ‚Ä¢ Choose strategy with highest Amdahl limit for better long-term scaling
  ‚Ä¢ IPC overhead is the primary bottleneck for Python multiprocessing
  ‚Ä¢ Shared-memory systems (C/C++/Go) would show significantly better scaling
  ‚Ä¢ Task-level parallelism has higher callback overhead but independent task execution

================================================================================
PHASE 3C: PARADIGM COMPARISON (Multiprocessing vs Concurrent.Futures)
================================================================================

Pixel-Level:
  Multiprocessing:      3.29x speedup
  Concurrent.Futures:   3.46x speedup
  Difference: -4.9%
  ‚Üí Nearly identical (same underlying pool mechanism)

Image-Level:
  Multiprocessing:      1.07x speedup
  Concurrent.Futures:   1.08x speedup
  Difference: -0.6%
  ‚Üí Nearly identical (same underlying pool mechanism)

Task-Level:
  Multiprocessing:      1.05x speedup
  Concurrent.Futures:   1.11x speedup
  Difference: -5.2%
  ‚Üí Concurrent.Futures performs better here

Key Trade-offs:
  Multiprocessing: Lower-level control, explicit pool management, apply_async callbacks
  Concurrent.Futures: Higher-level abstractions, cleaner API, as_completed() pattern
  Performance: <5% difference for CPU-bound tasks (both use process pools)
  Recommendation: Use concurrent.futures for new code (modern, pythonic)

================================================================================
PHASE 3D: THEORETICAL MODEL VALIDATION
================================================================================

Gustafson's Law (scaled problem size):
  Pixel-Level: p‚âà32.64% (if workload scaled with processors)
  Image-Level: p‚âà1.06% (if workload scaled with processors)
  Task-Level: p‚âà0.68% (if workload scaled with processors)

Validation Summary:
  ‚úì No superlinear speedup observed (expected for Python)
  ‚úì Efficiency degradation follows Amdahl's predictions
  ‚úì Serial portions align with IPC overhead measurements
  ‚úì Results consistent across both paradigms (validates implementation)

Saved data to ./output_images_test\benchmark_results_2.csv and ./output_images_test\benchmark_results_2.json

Generating Charts...
Chart saved to ./output_images_test\performance_1.png
